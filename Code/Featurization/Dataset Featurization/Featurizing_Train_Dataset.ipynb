{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"A100"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"OWy1xWA4K6CE"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["PATH = '/content/drive/MyDrive/1:1_Hanish_Acharla/Dataset/Final_Dataset/Train'"],"metadata":{"id":"Jo3m50NGLTjP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_optimizer(optimizer_name, learning_rate):\n","    # Import keras optimizers\n","    from tensorflow.keras.optimizers import Adam, Adadelta, Adagrad, Adamax, Ftrl, Nadam, RMSprop, SGD\n","    print('Selected Optimizer', optimizer_name)\n","    switcher = {\n","        'Adadelta': Adadelta(lr=learning_rate),\n","        'Adagrad': Adagrad(lr=learning_rate),\n","        'Adam': Adam(lr=learning_rate),\n","        'Adamax': Adamax(lr=learning_rate),\n","        'FTRL': Ftrl(lr=learning_rate),\n","        'NAdam': Nadam(lr=learning_rate),\n","        'RMSprop': RMSprop(lr=learning_rate),\n","        'Gradient Descent': SGD(lr=learning_rate)\n","    }\n","    # If optimizer_name is empty, Adam will be return as default optimizer\n","    return switcher.get(optimizer_name, Adam(lr=learning_rate))"],"metadata":{"id":"Zn_4GgxTLgMm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def convert_tf_dataset(PATH, model):\n","    # This function passes all images provided in PATH\n","    # and passes them through the model.\n","    # The result is a featurized image along with labels\n","    data = []\n","    IMG_SIZE = (224, 224)\n","    file_list = []\n","    # Get the list of subfolders\n","    sub_dirs = next(os.walk(PATH))[1]\n","    print(sub_dirs)\n","    num_images = 0\n","    # Create a list of lists\n","    # Number of lists is same as the number of subfolders\n","    # Number of items in the sub-list is the number of\n","    # images in each sub-folder\n","    for category in sub_dirs:\n","        files = next(os.walk(PATH + '/' + category), (None, None, []))[2]\n","        filenames = [PATH + '/' + category + '/' + file for file in files]\n","        num_images += len(filenames)\n","        file_list.append(filenames)\n","        labels = []\n","    # Every image is pre-processed and passed thought the model\n","    # Label is created for every image\n","    for category in file_list:\n","        for img_path in category:\n","            img = tf.keras.preprocessing.image.load_img(img_path, target_size=IMG_SIZE)\n","            img_array = tf.keras.preprocessing.image.img_to_array(img)\n","            img_batch = np.expand_dims(img_array, axis=0)\n","            img_preprocessed = preprocess_input(img_batch)\n","            data.append(model.predict(img_preprocessed))\n","            labels.append(img_path.split('/')[-2])\n","\n","    # Make sure dimensions are (num_samples, 1280)\n","    data = np.squeeze(np.array(data))\n","    labels = np.reshape(labels, (-1,1))\n","    return data, labels"],"metadata":{"id":"tjiRJYhvLrCl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Import packages needed to create a image classification model\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import os\n","import tensorflow as tf\n","from keras.applications.resnet import preprocess_input\n","#from keras.preprocessing.image import ImageDataGenerator # No change required here as it is not being used\n","from keras.layers import Dense,GlobalAveragePooling2D\n","from keras.models import Model\n","from keras.layers import Dense,GlobalAveragePooling2D\n","from keras.callbacks import EarlyStopping\n","from tensorflow import keras\n","IMG_SIZE = (224, 224)\n","# Download the model, valid alpha values [0.25,0.35,0.5,0.75,1]\n","base_model = tf.keras.applications.ConvNeXtXLarge(input_shape=(224, 224, 3), include_top=False, weights='imagenet')\n","# Add average pooling to the base\n","x = base_model.output\n","x = GlobalAveragePooling2D()(x)\n","model_frozen = Model(inputs=base_model.input,outputs=x)\n","# Get the transformed features from the dataset\n","# TODO: This can be moved to the FE stage of the pipeline\n","# label_map is not used anywhere right now. it has information\n","# about which label is mapped to which number\n","data, labels = convert_tf_dataset(PATH, model_frozen)\n","# Shuffle the dataset for training\n","shuffler = np.random.permutation(len(data))\n","data_shuffled = data[shuffler]\n","labels_shuffled = labels[shuffler]\n","print(data_shuffled)\n","num_features = data_shuffled.shape[1]"],"metadata":{"id":"_GdUFgImMHL_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","feature_names = []\n","for a in range(0,num_features):\n","  feature_names.append('feature_' + str(a))\n","feature_names.append('label')\n","df = pd.DataFrame(data=np.hstack((data_shuffled,labels_shuffled)), columns=feature_names)\n","df.head()"],"metadata":{"id":"Uz9vkgWCC0Qm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.to_csv('/content/drive/MyDrive/1:1_Hanish_Acharla/Dataset/Final_Dataset/Featurized DataSet/train_data.csv', index=False)\n"],"metadata":{"id":"ZZE05nduEDb-"},"execution_count":null,"outputs":[]}]}